
# Experiment Report: 4. **Meta-Learning for Hyperparameter Optimization

## Idea
4. **Meta-Learning for Hyperparameter Optimization**: Implement a meta-learning framework that rapidly tunes hyperparameters for new tasks by leveraging experience from previous tasks. This framework would minimize the need for extensive hyperparameter searches, thus reducing the computational cost and speeding up the training process.

## Experiment Plan
### 1. Objective
The primary objective of this experiment is to design, implement, and evaluate a meta-learning framework for hyperparameter optimization (HPO). This framework aims to leverage knowledge from previous tasks to rapidly tune hyperparameters for new tasks, thereby reducing computational costs and accelerating the training process.

### 2. Methodology
1. **Baseline Setup**:
   - Implement a traditional hyperparameter optimization approach such as grid search or Bayesian optimization.
   - Train models on a set of initial tasks using this traditional approach to gather a baseline performance metric.

2. **Meta-Learning Framework**:
   - Design a meta-learning algorithm that learns from the hyperparameters and performance metrics of previous tasks.
   - Implement a meta-model (e.g., a neural network or Gaussian Process) that predicts optimal hyperparameters for new tasks based on this learned knowledge.

3. **Training and Validation**:
   - Split the dataset into training and validation sets.
   - For each task in the training set, apply both traditional HPO and the meta-learning framework.
   - Compare the performance and computational cost of both approaches on the validation set.

4. **Testing**:
   - Apply the meta-learning framework to new, unseen tasks.
   - Measure how well the meta-learning framework predicts optimal hyperparameters and compare it against traditional HPO.

### 3. Datasets
- **Training and Validation**:
  - **MNIST** (Handwritten digits)
  - **CIFAR-10** (Image classification)
  - **SST-2** (Sentiment Analysis from the Stanford Sentiment Treebank)
  - **AG News** (News classification)

- **Testing**:
  - **Fashion-MNIST** (Clothing images)
  - **CIFAR-100** (Image classification with 100 classes)
  - **IMDB** (Sentiment analysis from movie reviews)
  - **TREC** (Question classification)

Datasets are available on Hugging Face Datasets.

### 4. Model Architecture
- **Image Classification**:
  - Convolutional Neural Networks (CNNs): LeNet-5 for simpler tasks, ResNet-18 for more complex tasks.
- **Text Classification**:
  - Recurrent Neural Networks (RNNs): LSTM or GRU.
  - Transformer-based models: BERT for more complex tasks.

### 5. Hyperparameters
- **CNN Hyperparameters**:
  - Learning Rate: [1e-4, 1e-3, 1e-2]
  - Batch Size: [32, 64, 128]
  - Number of Layers: [2, 4, 6]
  - Dropout Rate: [0.2, 0.4, 0.6]
  
- **RNN Hyperparameters**:
  - Learning Rate: [1e-4, 1e-3, 1e-2]
  - Batch Size: [32, 64, 128]
  - Number of Layers: [1, 2, 3]
  - Hidden Units: [128, 256, 512]
  - Dropout Rate: [0.2, 0.4, 0.6]

- **Transformer Hyperparameters**:
  - Learning Rate: [1e-5, 1e-4, 1e-3]
  - Batch Size: [16, 32, 64]
  - Number of Layers: [6, 12]
  - Number of Attention Heads: [8, 12]
  - Dropout Rate: [0.1, 0.2, 0.3]

### 6. Evaluation Metrics
- **Performance Metrics**:
  - Accuracy: Measure the proportion of correctly classified instances.
  - F1 Score: Harmonic mean of precision and recall, especially important for imbalanced datasets.
  - Mean Squared Error (MSE): For regression tasks if applicable.

- **Efficiency Metrics**:
  - Computational Time: Total time taken to complete hyperparameter optimization.
  - Number of Iterations: Number of HPO iterations required to reach optimal performance.
  - Computational Cost: Resource usage (e.g., GPU hours) during training and HPO.

- **Generalization Metrics**:
  - Performance on Unseen Tasks: Accuracy and F1 score on new tasks not seen during meta-learning.

This experiment is designed to comprehensively evaluate the effectiveness of a meta-learning framework for hyperparameter optimization, considering both its impact on model performance and its computational efficiency.

## Results
{'eval_loss': 0.432305246591568, 'eval_accuracy': 0.856, 'eval_runtime': 3.8989, 'eval_samples_per_second': 128.241, 'eval_steps_per_second': 16.158, 'epoch': 1.0}

## Benchmark Results
{'eval_loss': 0.698837161064148, 'eval_accuracy': 0.4873853211009174, 'eval_runtime': 6.3187, 'eval_samples_per_second': 138.003, 'eval_steps_per_second': 17.25}

## Code Changes

### File: model.py
**Original Code:**
```python
model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, num_classes)
)
```
**Updated Code:**
```python
model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, hidden_size2),  # Adding another hidden layer
    nn.ReLU(),
    nn.Linear(hidden_size2, num_classes)
)
```

### File: training.py
**Original Code:**
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
```
**Updated Code:**
```python
optimizer = torch.optim.Adam(model.parameters(), lr=0.0005)  # Reduced learning rate
```

### File: model.py
**Original Code:**
```python
model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Linear(hidden_size, num_classes)
)
```
**Updated Code:**
```python
model = nn.Sequential(
    nn.Linear(input_size, hidden_size),
    nn.ReLU(),
    nn.Dropout(p=0.5),  # Adding dropout layer
    nn.Linear(hidden_size, num_classes)
)
```

### File: training.py
**Original Code:**
```python
train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)
```
**Updated Code:**
```python
train_loader = DataLoader(dataset=train_dataset, batch_size=32, shuffle=True)  # Reduced batch size
```

## Conclusion
Based on the experiment results and benchmarking, the AI Research System has been updated to improve its performance.
